# pa-zhihu

## 环境

 - python 3.3以上
 - gevent
 - lxml
 - requests
 - 虽然我写了登陆界面，但是是抄的7sdream的，而且没有多次测试。所以现在登陆的cookie file是用7sdream的[zhihu-py3](https://github.com/7sDream/zhihu-py3)生成的。

## 计划

- 以一个用户的首页为起始页，开始爬用户关注的人。
- 得到每个用户的相关信息
- 得到数据之后聚合分析一下

## 步骤

- 拿到用户URL，从地址得到用户的唯一ID(知乎的)
- 用户首页得到除关注的人之外的所有信息
- 爬用户关注的人的列表, 爬到一堆人的URL
- 重复上面的动作

active level = 两个月内的活动数量，简单的相加，或者回答X3+提问X2+赞同X1

这个有点坑，要看回答，提问，点赞三个页面，而且点赞的页面还没有具体时间


valid level 主要是看是不是死人(比如葛巾...sad)或者马甲(僵尸号), 比如我的小号，就是被过滤的对象

比重依次为: 公共编辑, 收藏, 文章, 回答, 提问, 得到感谢, 关注者, 资料完整度(6个), 得到赞同,

暂时的计算方法是: [1.5, 1.4, 1.3, 1.2, 1.1, 1, 1, 1, 0.9],

等比较几个用户之后, 再给出一个标准分数线

~~活跃度这个最终还是放弃了，因为时效性太高，而这个项目的初衷是学习爬虫的并发-->得到定制化的数据-->实验数据库-->实验可视化~~


## MySQL

|zhihu-ID(varchar)|username(varchar)|location(varchar)|business(varchar)|gender(int-1,0,1)|company(varchar)|position(varchar)|education(varchar)|major(varchar)|agreed(int)|thanks(int)|answered(int)|asked(int)|posts(int)|collections(int)|public edition(int)|followed(int)|follower(int)|
|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|----|
|he-shi-jun|贺师俊| 上海 |互联网|1|--|--|五角场一流的复黏大学|自由而无用的哲学|18377|3415|758|16|4|3|100|171|18809|

## 最后

这个坑有点大, 而且现在还有点吹牛逼的嫌疑, 23333
我尽量做, 嗯嗯.


## 详细计划

- [ ] 实验gevent的性能
- [ ] gevent和multiprocessing结合使用的实验
- [ ] 应该维护两个MySQL表格吧，一个是上面的得到最终数据，一个是存放待爬的URL，存成自增的ID，知乎用户识别名称，和用户主页链接，状态标记。存入的状态标记是0，抓取成功并完成的时候删除，抓取一次失败就加1
- [ ] 看看程序运行中的状态检查是怎么实现的。运行中要能检查运行时间，效率，抓取用户数量，数据库还有多少未抓取用户。 
- [ ] 页面解析，并得到一个完整的上述表格
- [ ] 得到出示用户关注人的所有URL
- [ ] 【选作】得到初始用户的所有动态，并记录下爬的时间，以供之后对比使用。
- [ ] 看MySQL的插入和拆分怎么使用
- [ ] 先一次性抓完一个用户的连接，看看。然后开跑。

## 还存在的问题

- [ ] 初始化抓取的时候，需要等待第一次抓完
- [ ] 但是第一次抓完之后会直接打印Done，而我并没有设置超时
- [ ] 效率太低，好像给的8个协程一直没用满，大约2秒一个用户
- [ ] 数据写入数据库的时候，好像是同步的，数据量大的时候这个很明显
- [ ] 有时候会停在解析个人首页的状况，按ctrl+c之后就又继续了

## 下一步

- [ ] 如果要做社交网络的实验，还需要在每个人的个人字典中存入followed和follower的链接
- [ ] 试试redis
- [ ] 试试mongoDB